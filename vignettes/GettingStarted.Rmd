---
title: "Getting Started with dsims"
author: "L Marshall"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: refs.bib
vignette: >
  %\VignetteIndexEntry{dsims - getting started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Distance Sampling Simulations

This vignette introduces the basic procedure for setting up and running a distance sampling simulation using dsims [@dsims-pkg]. The dsims package uses the distance sampling survey design package dssd [@dssd-pkg] to define the design and generate the surveys (sets of transects). For further details on defining designs please refer to the dssd vignettes. dsims was designed to be largely similar to the DSsim package [@DSsim-pkg] in terms of work flow, functions and arguments. The main differences in terms of its use lie in the definition of the designs which can now be generated in R via the dssd package and the definition of analyses. Analyses are now defined using terminology based on the Distance package [@Distance-pkg]. In addition, the underlying functionality now makes use of the sf package [@sf-pkg] for improved efficiency.

Distance Sampling techniques provide design based estimates of density and abundance for populations, the accuracy of these estimates relies on valid survey design. While general rules of thumb can help guide our design choices, simulations emulating a specific set of survey characteristics can often help us achieve more optimal results for individual studies.

Simulations, for example, can help us maximise our precision across a range of possible scenarios, perhaps allocating effort in relation to a range of predicted density distributions. They can also help us investigate the possible effects of using a more efficient design but which may not have completely uniform coverage probability across the study region. The specifics of each of these scenarios and many others will be particular to the study in question and hence why simulation can be a powerful tool in survey design.

## Setting up the Region

We will use the St Andrews bay as an example study area for these simulations. This is a single strata study region which has been projected into metres. We will first load the dsims package, this will also automatically load the dssd package. Next we will obtain the location of the shapefile within the dssd design package which is then passed to the \code{make.region} function to create our study area. As this shapefile does not have a projection recorded (in an associated .prj file) we tell dsims that the units are metres.

```{r region, fig.align='center', fig.cap="Figure 1: The study region.", fig.width=3.8, fig.height=4}
library(dsims)
shapefile.name <- system.file("extdata", "StAndrew.shp", package = "dssd")
region <- make.region(region.name = "St Andrews bay",
                      shape = shapefile.name,
                      units = "m")
plot(region)
```


## Defining the study population

### Population Density Grid

The first step in defining your study population is to set up the density grid. One way to do this is to initially create a flat surface and then add hot and low spots to represent where you think you might have areas of higher and lower density of animals. We will first of all demonstrate this approach. 

Note that if we were to assume that there were 300 groups in the St Andrews bay study area (which is a fairly large number!) this would only give us an average density of 3.04-07 groups per square metre. For this simulation, as we will use a fixed population size, we do not need to worry about the absolute values of the density surface. Instead, it can be simpler to work with larger values and be aware that we are instead defining a relative density surface. So where we create a surface to have a density of twice that in another area that relationship will be maintained (be it at much smaller absolute values) when we later generate the population.

For the purposes of simulation you will likely want to test over a range of plausible animal distributions as if you knew exactly how many you were going to find at any given location you probably wouldn't be doing the study! When testing non-uniform coverage designs you may want to try out worst case scenarios, when density in the area of higher or lower coverage differs to majority of the survey region. This will give an idea of the degree of potential bias which could be introduced. In this example for the equal spaced zigzag design as it is generated in a convex hull the areas with differing coverage are likely to be at the very top and very bottom of the survey region. In the density grid below these areas are shown to have lower animal density than the rest of the survey region, a likely scenario when a study region has been constructed in order to catch the range of a population of interest.


```{r density, fig.align='center', fig.cap="Figure 2: A density map representing a plausible distributions of animals within the study region.", fig.width=4, fig.height=4}

# We first create a flat density grid
density <- make.density(region = region,
                        x.space = 500,
                        constant = 1)

# Now we can add some high and low points to give some spatial variability
density <- add.hotspot(object = density,
                       centre = c(-170000, 6255000),
                       sigma = 8000,
                       amplitude = 4)

density <- add.hotspot(object = density,
                       centre = c(-160000, 6275000),
                       sigma = 6000,
                       amplitude = 4)

density <- add.hotspot(object = density,
                       centre = c(-155000, 6260000),
                       sigma = 3000,
                       amplitude = 2)

density <- add.hotspot(object = density,
                       centre = c(-150000, 6240000),
                       sigma = 10000,
                       amplitude = -0.9)

density <- add.hotspot(object = density,
                       centre = c(-155000, 6285000),
                       sigma = 10000,
                       amplitude = -1)

# I will choose to plot in km rather than m (scale = 0.001)
plot(density, region, scale = 0.001)



```

In some situations you may not need to rely on constructing a density distribution from scratch. Below we will demonstrate how to use a gam to construct the density surface. As I do not have data for this area I will use the density grid I created above as an example dataset. I will fit a gam to this data and then use this to create a new density object. As I need to restrict the predicted values to be greater than 0 I will use a log link with the gaussian error distribution. This can also be a useful trick if you want to turn something created using the above method, which can look a bit lumpy and bumpy, into a smoother distribution surface. The gam fitted must only use a smooth over x and y to fit the model as no other predictor covariates will be present in the density surface.

```{r densitygam, fig.align='center', fig.cap="Figure 3: A density map representing a plausible distributions of animals within the study region.", fig.width=4, fig.height=4}

# First extract the data above - this is simple in this case as we only have a single strata
# Multi-strata regions will involve combining the density grids for each strata into a 
# single dataset.
density.data <- density@density.surface[[1]]
head(density.data)

# Fit a simple gam to the data
library(mgcv)
fit.gam <- gam(density ~ s(x,y), data = density.data, family = gaussian(link="log"))

# Use the gam object to create a density object
gam.density <- make.density(region = region,
                            x.space = 500,
                            fitted.model = fit.gam)

plot(gam.density, region, scale = 0.001)
```

### Other Population Parameters

Once we have created a plausible animal density distribution we can go on to define other population parameters. We do this by constructing a population description. 

We will assume animals occur in small clusters so we will first create a covariate list and define the distribution for cluster size (which must be named "size") as a zero-truncated Poisson distribution with mean equal to 3. For those of you familiar with DSsim please note the simplified format for defining population covariates. 

The other population value we have to define is the population size which now refers to the number of clusters, we will set this to 100. We then leave the fixed.N argument as the default TRUE to say we would like to generate the population based on the population size rather than the density surface.

```{r popdesc}

# Create a covariate list describing the distribution of cluster sizes
covariates <- list(size = list(distribution = "ztruncpois", mean = 3))

# Define the population description
pop.desc <- make.population.description(region = region,
                                        density = gam.density,
                                        covariates = covariates,
                                        N = 300,
                                        fixed.N = TRUE)

```

## Coverage Grid

It is good practice to create a coverage grid over your study area to assess how coverage probability varies spatially across your study area for any specified designs. For designs where there may be non-uniform coverage, we advise coverage probability is assessed prior to running any simulations. However, as this step is not essential for running simulations we will omit it here and refer you to the dssd vignettes for further details. 

## Defining the Design

dsims working together with dssd provides a number of point and line transect designs. Further details on defining designs can be found in the dssd help and vignettes. We also provide examples online at \url{https://examples.distancesampling.org/}.

For the purposes of these simulations we will compare two line transect designs, firstly systematically spaced parallel lines and secondly equal spaced zigzag lines. The zigzag design will be generated within a convex hull to try to minimise the off-effort transit time between the ends of transects. 

The design angles for each design were selected so that the transects run perpendicular to the coast. The way the two designs are defined meant that this was 90 degrees for the parallel line design and 0 for the zigzag design. Both designs assumed a minus sampling protocol and the truncation distance was set at 750m from the transect. The spacings for each design were selected to give the same trackline lengths of around 450 km (this was assessed by running the coverage simulations for these designs using \code{run.coverage}, see help in dssd). the trackline lengths can be thought of as an indicator of the cost of the survey as they give the total travel time (both on and off effort) from the beginning of the first transect to the end of the last transect.

```{r designs}
parallel.design <- make.design(region = region, 
                               design = "systematic",
                               spacing = 2500,
                               edge.protocol = "minus",
                               design.angle = 90,
                               truncation = 750)

zigzag.design <- make.design(region = region, 
                             design = "eszigzag",
                             spacing = 2233,
                             edge.protocol = "minus",
                             design.angle = 0,
                             bounding.shape = "convex.hull",
                             truncation = 750)

```


### Generating a Set of Transects

It is always a good idea to run a quick check that your design is as expected by generating a set of transects and plotting them. 

```{r seed, echo=FALSE}
set.seed(476)
```

```{r paralleltransects, fig.align='center', fig.cap="Figure 4: An example set of transects generated from the systematic parallel line design plotted within the study region.", fig.width=3.8, fig.height=4, fig.align='center'}
p.survey <- generate.transects(parallel.design)
plot(region, p.survey)
```

```{r zigzagtransects, fig.align='center', fig.cap="Figure 5: An example set of transects generated from the systematic parallel line design plotted within the study region.", fig.width=3.8, fig.height=4, fig.align='center'}
z.survey <- generate.transects(zigzag.design)
plot(region, z.survey)
```

## Defining Detectability

Once we have defined both the population of interest and the design which we will use to survey our population we now need to provide information about how detectable the individuals or clusters are. For this example we will assume that larger clusters are more detectable. Take care when defining covariate parameters that the covariate names match those in the population description.

When setting the basic scale parameter along side covariate parameters values we need be aware of how the covariate parameter values are incorporated. The covariate parameter values provided adjust the value of the scale parameter on the log scale. The scale parameter for any individual ($\sigma_j$) can be calculated as: 

$$\sigma_j = exp(log(\sigma_0)+\sum_{i=1}^{k}\beta_ix_{ij})$$
where $j$ is the individual, $\sigma_0$ is the base line scale parameter (passed in as argument \code{scale.param} on the natural scale), the $\beta_i$'s are the covariate parameters passed in on the log scale for each covariate $i$ and the $x_{ij}$ values are the covariate values for covariate $i$ and individual $j$.

We will assume a half normal detection function with a scale parameter of 400. We will set the truncation distance to be the same as the design at 750 m.  and set the covariate value on the log scale to log(1.05) = 0.049. We can check what our detection functions will look like for the different covariate values by plotting them. To plot the example detection functions we need to provide the population description as well as detectability. 

```{r detect, fig.align='center', fig.cap="Figure 6: Plot of the detection function for the mean group size (solid line) and for the 2.5 and 97.5 percentile values  of group size (dashed lines) for this population. ", fig.width=6, fig.height=4}

# Define the covariate parameters on the log scale
cov.param <- list(size = log(1.08))

# Create the detectability description
detect <- make.detectability(key.function = "hn",
                             scale.param = 300,
                             cov.param = cov.param,
                             truncation = 750)

# Plot the simulation detection functions
plot(detect, pop.desc)

```

We can also calculate the average detection function for our mean cluster size of 3 as defined in our population description:

$$\sigma_{size = 3} = exp(log(400)+log(1.05)*3) = 463.05 $$


## Defining Analyses

The final component to a simulation is the analysis or set of analyses you wish to fit to the simulated data. We will define a number of models and allow automatic model selection based on the minimum AIC value. The models included below are a half-normal with no covariates, a hazard rate with no covariates and a half-normal with cluster size as a covariate. We will leave the truncation value at 750 as previously defined (it must be <= to the truncation values used previously) and use the default error variance selection of "R2". See ?mrds::varn for descriptions of the various empirical variance estimators for encounter rate.

```{r analysis}

analyses <- make.ds.analysis(dfmodel = list(~1, ~1, ~size),
                             key = c("hn", "hr", "hn"),
                             truncation = 750,
                             er.var = "R2",
                             criteria = "AIC")

```

## Putting the Simulation Together

Now we have all the simulation components defined we can create our simulation objects. We will create one for the systematic parallel line design and one for the equal spaced zigzag design.

```{r simulation}

sim.parallel <- make.simulation(reps = 999,
                                design = parallel.design,
                                population.description = pop.desc,
                                detectability = detect,
                                ds.analysis = analyses)

sim.zigzag <- make.simulation(reps = 999,
                              design = zigzag.design,
                              population.description = pop.desc,
                              detectability = detect,
                              ds.analysis = analyses)

```

Once you have created a simulation we recommend you check to see what a simulated survey might look like. 

```{r parallel.survey, fig.align='center', fig.cap="Figure 7: Example survey from systematic parallel design. Panels showing: top left - transects, top right - population, bottom left - survey detections, bottom right -  detection distances", fig.width=6, fig.height=6}
# Generate a single instance of a survey: a population, set of transects 
# and the resulting distance data
eg.parallel.survey <- run.survey(sim.parallel)

# Plot it to view a summary
plot(eg.parallel.survey, region)

```


```{r zigzag.survey, fig.align='center', fig.cap="Figure 8: Example survey from equal spaced zigzag design. Panels showing: top left - transects, top right - population, bottom left - survey detections, bottom right -  detection distances", fig.width=6, fig.height=6}
# Generate a single instance of a survey: a population, set of transects 
# and the resulting distance data
eg.zigzag.survey <- run.survey(sim.zigzag)

# Plot it to view a summary
plot(eg.zigzag.survey, region)

```



## Running the Simulation

The simulations can be run as follows. 

```{r runsim, eval=FALSE}

# Running the simulations
sim.parallel <- run.simulation(sim.parallel)
sim.zigzag <- run.simulation(sim.zigzag)

```

## Simulation Results

As these simulations take some time to run I have already run them and stored the results. We can load these and view the simulation results.

```{r simresults, echo=FALSE}
load("files/sim.parallel.ROBJ")
load("files/sim.zigzag.ROBJ")


```

## Bibliography
